---
title: "**DESeq2 analysis: Method and Background**"   
author: "*Author:* Thomas Baardemans,   *Supervisor:* Marc Teunis"
date: "`r format(Sys.time(), '%d-%b-%Y')`"

output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    number_sections: true
    toc_depth: 3
    toc_title: "Table of Contents"
    code_folding: hide

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE, warning = FALSE, message = FALSE, collapse = TRUE)
```

```{r}
#BiocManager::install("airway")
```

```{r}
library(tidyverse)
library(DESeq2)
library(dplyr)
library(SummarizedExperiment)
library(airway)
library(here)
library(biomaRt)
library(gridExtra)
```

# **Introduction**
  
RNA sequencing is currently the most widely used technique for quantifying gene expression (transcriptome) in tissues, cell cultures or even individual cells. Interpretation of the resulting data is not as straightforward as it might seem due to biases introduced by the methods used and biological variety between samples. In order to correctly analyse a transcriptome dataset it is important to understand methods used to obtain and analyze the data. This lesson aims to give the student a broad overview of a typical RNA-seq experiment. Furthermore it will also give an insight into the statistical tests used by the DESeq2 package to test for differentially expressed genes. The lesson is split up in three chapters. The first chapter describes the setup of the experiment and what biases it may introduce and will also discuss the mapping of the reads to the genome. The second chapter discusses the inner workings of DESeq2 function and third chapter will focus on visualization of the results. 


# **Next Generations Sequencing and mapping reads**

Before we can begin to understand any of the statistical tests used by DESeq2 we need to have a rough understanding of the experiment that produced the data and steps involved. 


The technique uses reverse transcriptase to convert mRNA molecules to cDNA molecules. The resulting sequences(reads) of cDNA are mapped onto the genome. This produces a 'counts table' with the exact amount of mRNA molecules that were detected in the sample. This step of mapping the reads onto the genome is usually done already when 
a dataset is submitted to the GEO website. This process can be repeated byb downloading the .SRA files using the "SRA Run Selector" found on the GEO page.  
This step is not part of this course and we will start with the raw counts of each gene.

## Experimental design
Cell cultures can be harvested or biopts can be taken from animals or patients. The minimal design has at least 1 experimental variable and 3 replications (samples) per group. Careful consideration of replication number is advised though since a study using 42 replications per group has shown that using just 3 replications only identifies 20-40% of differentially expressed genes depending on the tool used (Schurch, Nicholas J et al. 2016). To identify >85% of genes regardless of fold change 20 replications are required (fold change is a quantative measure of difference. See chapter ...). They conclude at least 6-12 replications are recommended. 

## Library preparation.
After the RNA is extracted from the sample the ribosomal RNA (rRNA) needs to be removed. Over 90% of the RNA found in a cell is rRNA and only 1-2 % of the RNA is the mRNA we are interested in in a standard RNA-seq experiment (Conesa et al. 2016). mRNA selection can be achieved by selecting for the poly(A) tail found on eukaryotic mRNA's or by depleting rRNA. poly(A) selection yields better results but is not always possible in biopts and is never possible in bacteria because they do not have poly(A) tails in their mRNA's. poly(A) selection also filters out other non coding DNA such as microRNAs (miRNAs) because they do not contain a poly A tail either. After selection the mRNA's are fragmented and reverse transcripted to synthesize the cDNA library.     

```{r, echo=FALSE, include=TRUE, fig.align='center', out.width='100%', fig.cap="Figure x: cDNA library synthesis. adapted from: Overview of RNA seq tech from illumina website"}
knitr::include_graphics(path = here::here("images", "cDNA_library.png"))
```

## Next Generation Sequencing (NGS)  
Most NGS machines can only sequence molecules up to about 300 basepairs so the larger cDNA's need to be fragmentated first. Next the fragments are ligated to adapters to attach the molecules to the nanowells in the flowcell. NGS uses 1 strand of the cDNA to synthesize the complementing strand using fluorescently labelled nucleotides. After each cycle a new nucleotide attached and the flowcell is imaged.


Concept 1: Read depth (also know as sequencing depth) is a parameter with a different meaning depending on the experiment conducted. If a whole genome is sequenced the read depth is the number of times each nucleotide has been sequenced to call the base. In a typical whole genome experiment an average read depth of 30 to 50x is recommended by illumina (https://emea.illumina.com/science/technology/next-generation-sequencing/plan-experiments/coverage.html).
When using RNA-seq the read depth takes on a different meaning: It is the ammount of reads that is "sampled" from the whole population of cDNA molecules in the library (tarazona et al 2011). This usually runs in the millions depending on the goal of the experiment and can be > 100 million reads per sample. More read depth results in a more accurate quantification of genes that are expressed in low amounts. 

Further reading: For a more in-depth understanding of the NGS method you can watch this video: https://www.ibiology.org/techniques/next-generation-sequencing/#part-1 (short read NGS starts at 06:46 and ends at 20:52).

## Mapping reads and the challenges it poses
Now that the sequences of the reads are known they need to be mapped to a reference genome and counted. Usually the raw sequences of the reads are submitted to the Short Read Archive (SRA) so other researchers can reproduces every step that follows. Mapping the reads to the genome proves to be a harder than might be expected at first. One of the challenges is presented by the different isoforms of genes produced by alternative splicing. Over 90% of human genes are alternatively spliced producing a big amount of possible isoforms (stark_2019). This leads to reads being mapped ambiguously (fitting on more than one isoform) making it hard to count which read belongs to which isoform.   


```{r, echo=FALSE, include=TRUE, fig.align='center', out.width='100%', fig.cap="Figure x: Ambiguous reads present a problem in assigning a read to specific isoform. Even splice junction spanning reads can be ambiguous. Adapted from stark 2019"}
knitr::include_graphics(path = here::here("images", "stark2019_adapted.png"))
```

**@Marc: over het toeschrijven van reads aan bepaalde isoforms is me nog niet helemaal duidelijk want: Je zou in bovenstaande afbeelding naar de unambiguous reads kunnen kijken om de verhouding tussen de 2 isoforms te bepalen en dan alle ambiguous reads verdelen aan de hand van die verhouding over de 2 isoforms. Maar dat zou niet echt eerlijk zijn want de laatste exon van transcript y is een stuk langer dus die heeft een stuk meer reads. Wordt hiervoor ge-corrigeerd? Er wordt over het algemeen toch niet gecorrigeerd voor genlengte? Wil je sowieso dat ik ook behandel dat langere genen dus meer reads krijgen? Reads per kilobase per million reads (RPKM) of bij paired ends sequencing: Fragments per kilobase per million(FPKM)**

Paralogues reads prove to be an even harder problem to solve. Paralogues genes are genes that share a (relatively recent) common ancestor gene. This can lead to large portions of the sequence being completely identical. If a read falls within this zone it is impossible to know wether it belongs to gene x or y. Simply discarding these "multireads" can result in an underrepresentation of counts from genes that have lots of paralogues like the ubiquitin family for example. Some software(ERANGE) distributes these multireads in proportion to the number of unique and splice reads at similar loci (mortazavi 2008, laatste zin is bijna helemaal ge-copy-pasted van bron --> misschien nog even omschrijven). 

**@Marc: Alle unmapped reads die daarna nog overgebleven worden weggegooid toch?**

All the reads are stored in an expression matrix file (Also referred to as counts data or assay data). This expression matrix is sometimes also included in the expressionset uploaded to GEO but more often than not it is a seperate file included in the supplementary files. In other lessons of the course you will lean how to organize this expressionset into a neat dataset ready to be analyzed. For this lesson we will use a dataset called "airway" that can be downloaded/installed using bioconductor. 

```{r, eval = TRUE, include = TRUE}
# Uncomment and run the next line to install using bioconductor:

# BiocManager::install("airway")

#load the package into R
library(airway)

#load the dataset:
data(airway)

#There should now be an "airway" object loaded into your global environment. 
```

The airway dataset has already been cleaned up and formed into a SummarizedExperiment ready for a differential expression analysis. Before we start analyzing it is important to first explore the dataset to see what we are actually working with. 

Excercise 1: Inspect the airway dataset using the metadata(), assay() and colData() functions from the SummarizedExperiment package. Use these functions to answer the following questions:

- What kind of cells have been used in the experiment? 
- How does the assaydata relate to the coldata?
- What are the experimental conditions? 

open the codeblock to see the answers. 
```{r}
# Answer 1: airway smooth muscle cells (use metadata() function to see title and link to pubmed).

# Answer 2: The columnnames of the assaydata are linked to rownames of the colData. So each row in the colData contains information about a sample (column in assaydata). 

# Answer 3: Dex and cell. Read the pubmed abstract to get more info about dex. Because this dataset is only a subset of a bigger dataset there is also a variable called albut. All the samples untreated so its not an experimental group in our subset of the data. 
```

Further reading: To understand the algorithm used to map reads to the genome while making efficient use of computing resources (Burrows-Wheeler transform) watch this lecture by MIT professor David Gifford: https://www.youtube.com/watch?v=P3ORBMon8aw

# **Differential gene expression analysis**

The mapping of the reads results in a counts table. This table reports the amount of times a read was mapped to a specific gene. These numbers quantify linearly to the abundance of mRNA in the sample (Mortazavi, A., Williams, B., McCue, K. et al. 2008). 
Before we can run statistical test on the data there are a few steps to go through. These steps are included in the DESeq2 function. DESeq2 is basically a "wrapper" function that executes a number of other functions in the "background". The purpose of these functions is roughly the following:

- Estimation of size factor 
- Estimation of dispersion
- Testing 

In this chapter we will go through each step in turn and use the "airway" dataset to see what happens to the data throughout the analysis. 


## "Normalizing" the data (First step of DESeq2: Estimation of size factor)

When comparing the readcounts of a gene between 2 experimental conditions we need to make sure that the difference is only influenced by the treatment and not by any other biases. 
Lets take a look at the "airway" dataset. This dataset contains the summarized experiment from an experiment that treated airway smooth muscle cells with dexamethasone (glucocorticosteroid). 

```{r, echo = TRUE}

#Lets take a look at some of the assaydata
head(assay(airway))

#some of the samples (columns in assaydata) are treated and some are not. To figure out which is which we take a look at the coldata
colData(airway)
```

Now we know what is what we would like to know if both conditions have an equal amount of reads in them.

```{r, echo = TRUE}
#total reads per treatment.
sum(assay(airway[,airway$dex == "trt"]))
sum(assay(airway[,airway$dex == "untrt"]))

```

The difference is not huge. But its still 3.6 million reads less in the treated group. Could it be that the treated group has some genes downregulated? Or is something else going on?
The difference becomes more apparent if we take a look at the individual samples (fig x). The total amount of reads ranges from 15 million to over 30 million within the treated condition! This difference can be due to multiple reasons: Conditions in the well might have been slightly different leading to quicker growth or the samples might nog have an equal sequencing depth. 

```{r}
reads_per_sample <- data.frame(reads = c(sum(assay(airway[,1])), 
                                         sum(assay(airway[,2])),
                                         sum(assay(airway[,3])),
                                         sum(assay(airway[,4])),
                                         sum(assay(airway[,5])),
                                         sum(assay(airway[,6])),
                                         sum(assay(airway[,7])),
                                         sum(assay(airway[,8]))),
                               dextreatment = colData(airway)$dex 
                                )  
```


```{r}
ggplot(data = reads_per_sample, mapping = aes(x = 1:8, y = reads, fill = dextreatment)) +
  geom_col()
```
  
 
In order to adjust for this effect you might propose to scale all reads so the total amount of reads will become equal between samples. 

**verhaal over regression komt hier zie figuur hieronder**

```{r, echo=FALSE, include=TRUE, fig.align='center', out.width='50%', fig.cap="Figure x: estimation of size factor. source: mod stat mod bio chapter 8 fig 8.1."}
knitr::include_graphics(path =here::here("images", "chap7-rnaseq-normalization-1.png"))
```


DESeq2 is a wrapper that will run multiple functions. The first of which is the "estimate size factors" function and it can also be run manually:
```{r, echo = TRUE}
estim <- DESeq2::estimateSizeFactorsForMatrix(assay(airway))
```

```{r}
reads_per_sample <- mutate(reads_per_sample, 
                           sizefactor = estim)

reads_per_sample <- mutate(reads_per_sample,
                           reads_adjust = reads_per_sample$reads/reads_per_sample$sizefactor)

```




```{r}
ggplot(data = reads_per_sample, mapping = aes(x = 1:8, y = reads_adjust, fill = dextreatment)) +
  geom_col()
```


```{r}


normalized_assay <- data.frame("SRR1039508" = assay(airway[,1])/1.0236476,
                               "SRR1039509" = assay(airway[,2])/0.8961667,
                               "SRR1039512" = assay(airway[,3])/1.1794861,
                               "SRR1039513" = assay(airway[,4])/0.6700538,
                               "SRR1039516" = assay(airway[,5])/1.1776714,
                               "SRR1039517" = assay(airway[,6])/1.3990365,
                               "SRR1039520" = assay(airway[,7])/0.9207787,
                               "SRR1039521" = assay(airway[,8])/0.9445141)
normalized_assay <- round(normalized_assay)



```

As you can see the range of total reads has drastically decreased. All samples now have between 20 and about 22,5 million total reads.  

Note: if you look at the help page of estimateSizeFactorsForMatrix you might notice the function also accepts an optional argument called controlGenes. 
The control genes can come in 2 forms: 

- Spike-in genes: These are artificial mRNA sequences that can be added in a known quantity to the sample.
- Housekeeping genes: These genes are essential for basic maintenance and function of the cell. Some of these are expressed at relatively constant rates. 


```{r eval = FALSE, include = TRUE}
#housekeeping genes
# dds_adjusted_airway <- DESeq2::DESeqDataSetFromMatrix(assay(adjusted_airway),
#                                             colData = colData(adjusted_airway),
#                                             design=~dex+cell)


#quick biomart anotating
ensembl <- useMart("ensembl")

ensembl = useDataset("hsapiens_gene_ensembl",mart=ensembl)

annot <- getBM(attributes= c('ensembl_gene_id', 'external_gene_name'), 
               mart = ensembl)



normalized_assay <- tibble::rownames_to_column(normalized_assay, "ensembl_gene_id")
normalized_assay <- left_join(normalized_assay, annot, by = "ensembl_gene_id")
head(normalized_assay)



#some housekeeping genes proposed for calibration by Eisenberg_Levanon_2013
housekeeping_genes <- c("VCP", "EMC7", "REEP5", "PSMB2")
normalized_assay[which(normalized_assay$external_gene_name %in% housekeeping_genes),]

```

Note: although this step is often called normalization it actually does not have anything to do with normal distribution. 

## Estimating dispersion (second step of DESeq2)  

When a variable has a countable number of values it is said to be a discrete value.
These values follow discrete probability distributions.
 

## Binomial distribution

The simplest example of a binomial distribution is the coin toss. binomial distribution works with boolean values: true/false, 0 or 1, fail or succes. Each "cointoss" is called a Bernoulli trial. 

```{r, echo = TRUE}
#randomly generate 10 cointosses
rbinom(n = 10, size = 1, prob = 0.5)
```

exercise: Use rbinom to simulate 100 double rolls of a die. Where throwing 6 is considered "succes" and everything below a "fail". The results should show a "1" when a single 6 has been rolled and "2" when both die rolled a 6. 

```{r, echo = TRUE}
#Answer:
rbinom(100, 2, prob = 1/6)
```

To get complete distribution of probabilities we use dbinom. 
note: use rbinom to generate data (r stands for random in this case). use dbinom to generate a probability (d stands for distribution?). This also works with other statistical distributions as we will see later on in the lesson. 
```{r}
#probability that 6 is thrown twice.
dbinom(2, 2, prob = 1/6)

#to get the probabilities for all possible outcomes we use the colon. the chance of 3 succes if ofcourse zero because we only throw with 2 dice.
dbinom(0:3, 2, prob = 1/6)


barplot(dbinom(0:3, 2, prob = 1/6), names.arg = 0:3)
```

## Poisson distribution 

*When the probability of success p is small and the number of trials n large, the binomial distribution B(n,p) can be faithfully approximated by a simpler distribution, the Poisson distribution with rate parameter λ=np.*
**(Stuk tussen haakjes is copy pasta uit mod stat mod bio. Nog even omschrijven. Maar is wel mooi omschreven.)** 

```{r, fig.cap="figure1: A barplot showing a poisson distribution. k = number of occurences, P(X=k) = probability of succes given lambda"}
barplot(dpois(x = 0:12, lambda = 6), names.arg = 0:12, col = "#2780e3", xlab = "k", ylab = "P(X=k")
```

If we look at figure 1 there is an event with a $\lambda$ of 6. $\lambda$ might represent the number of letters someone has received on average each day for the last month. If we were to look at the mail that arrived on a given day there is a chance of 0.04 (P) of us counting 2 (k) pieces of mail. 

A good example of a Poisson distribution is the mutation rate of DNA. If we want to know the amount of mutations we can we are likely to find in 1 kb of DNA we start with determining the $\lambda$ (event rate). In this hypothetical example we examine 1 mb and find 312 mutations. So $\lambda$ = 312/1000 = 0.312. To calculate the chance we find 3 mutation in 1 kb of DNA we can use the following poisson formula 

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <mi>X</mi>
  <mo>=</mo>
  <mi>k</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <msup>
        <mi>&#x03BB;<!-- λ --></mi>
        <mi>k</mi>
      </msup>
      <mspace width="thickmathspace" />
      <msup>
        <mi>e</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>&#x2212;<!-- − --></mo>
          <mi>&#x03BB;<!-- λ --></mi>
        </mrow>
      </msup>
    </mrow>
    <mrow>
      <mi>k</mi>
      <mo>!</mo>
    </mrow>
  </mfrac>
  <mo>.</mo>
</math>

So in this formula we input $\lambda$ = 0.312, $\kappa$ = 3. Note that the ! represents a factorial: n! = n * (n-1) * (n-2) ...
So our k! = 3*2*1 = 6. In R we can use this formula as follows: 

```{r, echo = TRUE}
0.312^3 * exp(-0.312) / factorial(3)

# However there is a function in R that will work equally well:

dpois(x = 3, lambda = 0.312)

```

so there is a probability 0.0037 that a given kb contains 3 mutations. So we expect to find about 4 sections of DNA to contain 3 mutations. 
To get P(X=k) for all the other k values we can change the code slightly:

```{r}
dpois(x = 0:4, lambda = 0.312)
```

and plot these values
```{r}
barplot(dpois(x = 0:4, lambda = 0.312), names.arg = 0:4, col = "#2780e3", xlab = "k", ylab = "P(X=k")
```


results/"random" generation. **Misschien is het leuker om een stuk DNA uit het eerste gedeelte van de cursus te gebruiken en die met biostrings te analyseren of iets?**
```{r}
table(rpois(1000, 0.312))
```


## Exercise: V1 flying bomb. Guided weapon or probably just poisson distribution? 

```{r, echo=FALSE, include=TRUE, fig.align='center', out.width='100%', fig.cap="Figure 1: Illustration of a V1 flying bomb. The V stands for 'vergeltungswaffen' (vengeance weapon) and was used to terrorize London civilians in the second world war."}
knitr::include_graphics(path =here::here("images", "V1_cutaway.JPG"))
```

Poisson distributions occur in all kinds of unexpected places. During World War 2 the german army developed a flying bomb (V1 and later V2) to terrorize civilians in London. The impact sites of these bombs tended to be grouped in clusters. This made the English wonder whether these clusters were the result of some aiming mechanism on the bombs or if it was just chance. Statistician R. D. Clarke set out to tackle this problem (clarke1946). He selected a 144 km^2^area on the map of south London and divided it in 576 squares of 0,25 km^2^. Counting the recorded impacts over the last x months Clarke determined there had been 537 impacts of flying bombs in the selected area.    

**Question 1: Use the information given above to calculate the probabilities that a given square had 0,1,2,3,4 or 5+ impacts if the impacts were distributed according Poisson probabillity. Click the "code" button to reveal the answer.**


```{r, echo = TRUE}
#First we calculate the chance that a given square got bombed 0,1,2,3 or 4 times.
zero_to_four <- dpois(x = 0:4, lambda = 537/576)

#The chance a square gets impacted 5 or more times is 1 minus the sum of chances it gets hit 0 to 4 times. 
five_plus <- 1-sum(zero_to_four)

#Concatenating the chances.
poissonexpectation <- c(zero_to_four, five_plus)

#Show the complete vector of probabilities (from 0 to 5+):
poissonexpectation
```
 
Next Clarke counted the number of squares that contained 0,1,2,3,4 and 5+ bomb impacts. Clarke hypothesized that if the V1 flying bombs contained an aiming mechanism there would be a disproportionate amount of squares containing either relatively high number of impacts or none at all. 

```{r}
options(digits = 3)
knitr::kable(data.frame("impacts per square" = c("0", "1", "2", "3", "4", "5+"),
                        "expected amount of squares" = poissonexpectation*576,
                        "actual amount of squares" = c(229, 211, 93, 35, 7, 1)), 
             align = "c", row.names = FALSE, 
             col.names = c("impacts per square", "expected amount of squares", "actual amount of squares"),
             caption = "R.D. Clarke's results")
options(digits = 10)
```

As you can see from just looking at the results the poisson distribution predicted the actual distribution quite effectively. Te verify that the bomb impacts follow a poisson distribution we can perform a chi-squared test using the chisq.test function in R. 

```{r}
bombdata <- c(229, 211, 93, 35, 7, 1)

# chi square testing 
chisq.test(x = bombdata, p = poissonexpectation)

```

This result is in line with what we know about the V1 today: It could only be aimed in the general direction of London but not guided to a more specific target like a building or a block of buildings. 

## Gamma-Poisson: negative binomial distribution

Now we have seen how a distribution can help in predicting the likelihood a measurement was fo

It turns out the Poisson distrubution alone does not accurately reflect the distribution of genes. The main problem is the lambda parameter the in poisson distributions: lambda = mean = variance. 
To accurately model the data we need a distribution that uses seperate parameters for mean and variance (dispersion).  

Estimating dispersion of a gene is hard/impossible if you only have 3 samples per experiment group because n is too small. You would need quite alot of samples to accurately preditc the dispersion and quite often you are working with data gathered by someone else so this is simply not an option. As a solution DESeq2 assumes genes of similar average expression have a similar dispersion (Love et al 2014). 

```{r, echo=FALSE, include=TRUE, fig.align='center', out.width='100%', fig.cap="Figure x: Plot of dispersion estimates of airway dataset. The black dots represent the dispersion calculated for a given gene (gene-wise estimate). the average dispersion of genes with simular expression (mean of counts) is calculated. A red line is fitted trough the average dispersions and the genes are shrunken toward the red line resulting in the blue dots. The blue dot represent the final estimated dispersion of a gene. The thicker blue dots in the uppermost part of the graph are considered outliers and for this reason they are not shrunk towards the fitted value."}

ddsSE_airway <- DESeq2::DESeqDataSet(airway, design =  ~dex)
keep <- rowSums(counts(ddsSE_airway)) >= 10
ddsSE_airway <- ddsSE_airway[keep,]
ddsDE_airway <- DESeq(ddsSE_airway)
plotDispEsts(ddsDE_airway)
```



```{r}
dispersiondataframe <- data.frame(assay(ddsDE_airway)) %>%
  mutate(disp = dispersions(ddsDE_airway))

head(dispersiondataframe) 

#use view(dispersiondataframe) and pick any gene you want. Here we choose ENSG00000033867
barplot(assay(subset(airway, rownames(airway) %in% "ENSG00000033867")))

#negative binomial
rnbinom(4, size = (0.027 * 865), mu = 865)

sample <- rnbinom(3, size = 51, mu = 1000)

qnbinom(sample, size = 51, mu = 1100)



# mu = mean, size = dispersion. 
barplot(dnbinom(0:200, size = 10, mu =  100))
```




## Log2 fold change and shrinkage

After running DESeq2 and taking a look at the results you might notice a column for the log2FoldChange and one for the lfcSE.
Log2FoldChange is a parameter that is calculated to show the relative difference in expression. Regular expression amounts of genes varies wildly from a couple of counts up to 400.000 in the airway dataset. This makes hard to visualize the increase or decrease of expression. So lets make a hypothetical dataset and to experiment on:

```{r}
df <- data.frame(genes = c("gene1", "gene2", "gene3", "gene4"),
                 sample1 = c(1000,23,230000,7523),
                 sample2 = c(8000,85,200000,3250))
df
```

Now in order to quickly view how much each gene has changed we calculate the "fold change". For example: If sample 1 is the "before treatment" and sample 2 is "after treatment". We simply take the expression of sample 2 and divide it by the expression of sample 1. If the result is >1 the gene is upregulated after treatment. 

```{r}
df <- mutate(df, 
             foldchange = df$sample2 / df$sample1)

df

ggplot(df, aes(x = genes, y = foldchange)) +
  geom_point()

```

Now this graph shows the fold change for all the upregulated genes are >1 and all the downregulated genes are between 0 an 1. 
These downregulated genes are a problem now because its still hard to compare upregulation and downregulation. 

The solution is taking the log2 of the fold change:

```{r}
df <- mutate(df, 
             log2foldchange = log2(df$foldchange))

df

ggplot(df, aes(x = genes, y = log2foldchange)) +
  geom_point()

```
notice that due to the logarithmic scaling a gene with a fold change of 8 has a log2fold change of 3 because 2x2x2 = 8.  

The result is a much clearer graph of relative differences in gene expression. 

The other parameter in the DESeq2 results is the lfcSE. This is short for Log2FoldChange Shrinkage Estimate. 
Shrinking the data is done because genes with low counts have less "statistical information". 

```{r}
data(airway)
ddsSE_airway <- DESeq2::DESeqDataSet(airway, design =  ~dex)
keep <- rowSums(counts(ddsSE_airway)) >= 10
ddsSE_airway <- ddsSE_airway[keep,]
ddsDE_airway <- DESeq(ddsSE_airway)
res <- results(ddsDE_airway)

resultsNames(ddsDE_airway)
resLFC_airway <- lfcShrink(ddsDE_airway, coef="dex_untrt_vs_trt", type="apeglm")

DESeq2::plotMA(res, main = "before lfc shrinkage", alpha = 0.05)
DESeq2::plotMA(resLFC_airway, main = "after lfc shrinkage", alpha = 0.05)
```


# Test voor transcript length bias:


```{r, echo=FALSE, include=TRUE, fig.align='center', out.width='100%', fig.cap="Figure x) A: Transcript length bias. Average bp length of all possible isoforms was calculated per gene. Genes were arranged by length and binned per 500 genes and the average genelength of each bin was calculated. The Y axis show the fraction of genes per bin that were differentially expressed. The DESeq2 results turn out to be slightly skewed towards the longer genes. B: After performing a log fold shrink the bias becomes magnified."}
#annotate airway dataset with transcript length to check for transcript length bias. 
data(airway)
ddsSE_airway <- DESeq2::DESeqDataSet(airway, design =  ~dex+cell)
keep <- rowSums(counts(ddsSE_airway)) >= 10
ddsSE_airway <- ddsSE_airway[keep,]
ddsDE_airway <- DESeq(ddsSE_airway)

#biomart
res <- results(ddsDE_airway)

res$ensembl_gene_id <- rownames(res)

ensembl <- useMart("ensembl")

ensembl = useDataset("hsapiens_gene_ensembl", mart=ensembl)

annot <- getBM(attributes= c('ensembl_gene_id', 'transcript_length'), 
               mart = ensembl)

annot <- annot %>%
  group_by(ensembl_gene_id) %>%
  summarise(transcript_length = mean(transcript_length)) %>% 
  arrange(ensembl_gene_id)

res <- left_join(as.data.frame(res), annot, by = "ensembl_gene_id")

res <- arrange(res, transcript_length) %>%
  filter(transcript_length < 10000)

allbins <- seq(0, 21000, 500)

percentagecalculator <- function(bin){
  binres <- res[bin:(bin+499),]
  perc_sig <- filter(binres, padj < 0.05)
  npercres <- nrow(perc_sig)
  percentage <- (npercres/nrow(binres))
return(percentage)
}

lengthcalculator <- function(bin){
  binres <- res[bin:(bin+499),]
  avg_length <- (sum(binres$transcript_length)/500)
return(avg_length)
}

percentage_de <- sapply(allbins, percentagecalculator)

avg_length_de <- sapply(allbins, lengthcalculator)

length_bias <- data.frame(lengthoftranscript = avg_length_de,
           percentage = percentage_de)


graph1 <- ggplot(data = length_bias, aes(x = lengthoftranscript, y = percentage)) + 
  geom_point() + 
  ylim(0,0.35) +
  xlab("length of transcript (average bp of isoforms) ") + 
  ylab("fraction differentially expressed (p < 0.05)")

res <- lfcShrink(ddsDE_airway, coef="dex_untrt_vs_trt", type="apeglm")

res$ensembl_gene_id <- rownames(res)

res <- left_join(as.data.frame(res), annot, by = "ensembl_gene_id")

res <- arrange(res, transcript_length) %>%
  filter(transcript_length < 10000)

percentage_de <- sapply(allbins, percentagecalculator)

avg_length_de <- sapply(allbins, lengthcalculator)

length_bias <- data.frame(lengthoftranscript = avg_length_de,
           percentage = percentage_de)

graph2 <- ggplot(data = length_bias, aes(x = lengthoftranscript, y = percentage, xlab = "lala")) + 
  geom_point() + 
  ylim(0,0.35) +
  xlab("length of transcript (average bp of isoforms) ") + 
  ylab("fraction differentially expressed (p < 0.05)")

grid.arrange(graph1, graph2, ncol=2)
```



# Exploring Results (histogram of p-values, MA/vulcan plot, PCA plot and Heatmap) 
```{r}
#test codechunk
barplot(assay(subset(airway, rownames(airway) %in% "ENSG00000179094")))
```


```{r}
# hoe het eigenlijkzou moeten die annotatie
annot <- annot[match(rownames(ddsDE_airway), annot$ensembl_gene_id),]

all(rownames(ddsDE_airway) == annot$ensembl_gene_id)

```




